<div class="m-body">
  <div class="modal-exit" ng-click="closeModal()"><i class="fa fa-times"></i></div>
  <div class="modal-main">
    <div class="modal-title" style="background-color:#FBD774">{{model.active_q.text}}</div>
    <p>Machine learning (ML) techniques are becoming increasingly popular in DH. However, these techniques were originally designed with certain assumptions about the kinds of data they would be used on. <a href=”https://doi.org/10.1093/llc/fqn019” target=”_blank”>Literary DH scholars D. Sculley and Bradley Pasanek argue that computational learnability theory, which guarantees the learnability of arbitrary concepts, makes assumptions that are at odds with those of humanistic knowledge production</a>.</p>
    <p>The first assumption, for example, is that  "data is produced by some process with constant probabilistic qualities", which crucially does not change over time, and that examples can be drawn from it identically and independently. This assumption is what enables formal generalisation and creates the conditions for the proof that a hypothesis learned on early data can be used to make predictions about later data. It is, however, an assumption that rarely holds with respect to humanities data; historians and scholars of culture, for instance, are frequently interested in change over time -- they <i>fully expect</i> the dataset they are working to exhibit some non-constant behaviour. The required existence of a fixed distribution  also “implies that there is, at some level, a fixed truth” which is a troubling notion in a humanistic context -- as <a href="https://www.jstor.org/stable/pdf/10.5749/j.ctt1cn6thb.46.pdf?acceptTC=true&coverpage=false&addFooter=false" target="_blank">Stephen Ramsey describes</a>,  “the ethics of humanistic inquiry demands that we treat our questions as always being fundamentally rhetorical in nature if only as a way to respect the complexity of human culture”. </p>
    <p>The second requirement is that, in order to guarantee generalisation, the hypothesis space -- th “set of all possible concepts that the learning method might produce” --must be restricted. This means that the experimenter must pick a single ‘best’ way of formulating a hypothesis. Do they want to try to prove their point by creating a binary classification? Or by showing the predictive capacity of the examples? Or something else altogether? Each of these would require a different learning algorithm, but with complex humanities problems it can be hard to one, to pick the best hypothesis space before trying anything and, two, to explain the reasoning that led to this choice. In this case, experimenter biases have a huge impact on the result but are easily invisibilised. </p>
    <p>The third requirement is that the observed phenomenon must be ‘well represented’, with the data reflecting its most salient properties.. However, in the humanities,  data is understood as something that is actively constructed so, according to their aims and individual contexts -- there correspondingly cannot be a single agreed up understanding of ‘well represented’.</p>
    <p>In the course of their experiments on the application of data mining techniques to investigate the proposed link between metaphorical language and political affiliation, Sculley and Pasanek were able to produce startlingly different precisions and recall values—ranging from  around 70% to just above the 33% random baseline—on the same dataset by using different combinations of models, cross-validation divisions, and feature derivations. Their conclusion is not that it is counterproductive to employ machine learning in humanistic contexts, but rather that the results cannot be considered as independent proof but must rather be integrated into a larger contextualised discussion that notably attends to the implications of the limitations of the models. </p>
  </div>
</div>
